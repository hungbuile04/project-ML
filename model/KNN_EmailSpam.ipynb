{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClBDMf6HQKE5",
        "outputId": "3b8a5eb5-6fe8-4eac-cab9-5817bf24b584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed gdown-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown==4.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data google drive link c·ªßa H√πng\n"
      ],
      "metadata": {
        "id": "KT_mjqwnQ-Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "!mkdir -p content/data\n",
        "# Download all files\n",
        "file_ids = [\n",
        "    '1JXdyh-UVveXhBToLJj8ZJNttiJif6zH8',  # X_train\n",
        "    '1rtyLyNDBr3sPcyElMhpDPK3qTbNZH1YT',  # X_test\n",
        "    '1IkNvEhPFfano0qSDZFWhsmpF3ttC1q0Y',  # y_train\n",
        "    '1JlMWumYQP9OIH79dVsCiZ-eavZAhgSzv'   # y_test\n",
        "]\n",
        "\n",
        "file_paths = [\n",
        "    'content/data/X_train.csv',\n",
        "    'content/data/X_test.csv',\n",
        "    'content/data/y_train.csv',\n",
        "    'content/data/y_test.csv'\n",
        "]\n",
        "for file_id, file_path in zip(file_ids, file_paths):\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', file_path, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5eKWUVbRCw0",
        "outputId": "71b6bec6-437a-4837-bc07-797df2b220f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JXdyh-UVveXhBToLJj8ZJNttiJif6zH8\n",
            "To: /content/content/data/X_train.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5.03M/5.03M [00:00<00:00, 64.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rtyLyNDBr3sPcyElMhpDPK3qTbNZH1YT\n",
            "To: /content/content/data/X_test.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.34M/1.34M [00:00<00:00, 20.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IkNvEhPFfano0qSDZFWhsmpF3ttC1q0Y\n",
            "To: /content/content/data/y_train.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 16.9k/16.9k [00:00<00:00, 25.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JlMWumYQP9OIH79dVsCiZ-eavZAhgSzv\n",
            "To: /content/content/data/y_test.csv\n",
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4.24k/4.24k [00:00<00:00, 6.96MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ki·ªÉm tra data"
      ],
      "metadata": {
        "id": "JRLI49vgR5VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ƒê·ªçc file CSV\n",
        "import pandas as pd\n",
        "df_x_train = pd.read_csv('content/data/X_train.csv')\n",
        "df_x_test = pd.read_csv('content/data/X_test.csv')\n",
        "df_y_train = pd.read_csv('content/data/y_train.csv')\n",
        "df_y_test = pd.read_csv('content/data/y_test.csv')\n",
        "\n",
        "# Hi·ªÉn th·ªã th√¥ng tin v√† c·ªôt\n",
        "print(\"\\nüü¶ X_train:\")\n",
        "print(df_x_train.info())\n",
        "print(\"T√™n c·ªôt:\", df_x_train.columns.tolist())\n",
        "\n",
        "print(\"\\nüü® X_test:\")\n",
        "print(df_x_test.info())\n",
        "print(\"T√™n c·ªôt:\", df_x_test.columns.tolist())\n",
        "\n",
        "print(\"\\nüü© y_train:\")\n",
        "print(df_y_train.info())\n",
        "print(\"T√™n c·ªôt:\", df_y_train.columns.tolist())\n",
        "\n",
        "print(\"\\nüü• y_test:\")\n",
        "print(df_y_test.info())\n",
        "print(\"T√™n c·ªôt:\", df_y_test.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOpT8T2zR7jT",
        "outputId": "69a7058d-6c7a-4d30-bff3-ca7658fb3bde"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "üü¶ X_train:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8457 entries, 0 to 8456\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   message  8445 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 66.2+ KB\n",
            "None\n",
            "T√™n c·ªôt: ['message']\n",
            "\n",
            "üü® X_test:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2115 entries, 0 to 2114\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   message  2114 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 16.7+ KB\n",
            "None\n",
            "T√™n c·ªôt: ['message']\n",
            "\n",
            "üü© y_train:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8457 entries, 0 to 8456\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype\n",
            "---  ------    --------------  -----\n",
            " 0   category  8457 non-null   int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 66.2 KB\n",
            "None\n",
            "T√™n c·ªôt: ['category']\n",
            "\n",
            "üü• y_test:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2115 entries, 0 to 2114\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype\n",
            "---  ------    --------------  -----\n",
            " 0   category  2115 non-null   int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 16.7 KB\n",
            "None\n",
            "T√™n c·ªôt: ['category']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOWs"
      ],
      "metadata": {
        "id": "j2H2NNjqxYmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Thay ƒë·ªïi t·ª´ TfidfVectorizer sang CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# 2. T·∫£i d·ªØ li·ªáu t·ª´ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
        "# Ch√∫ √Ω c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ƒë√∫ng n·∫øu s·ª≠ d·ª•ng file c·ªßa b·∫°n\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 4. X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
        "# L√†m s·∫°ch d·ªØ li·ªáu\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')  # X·ª≠ l√Ω NaN\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 5. M√£ h√≥a nh√£n\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 6. H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n n√¢ng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization & remove non-alpha\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 7. Chuy·ªÉn sang Bag of Words (BOW)\n",
        "vectorizer = CountVectorizer(max_features=17, stop_words='english')  # S·ª≠ d·ª•ng CountVectorizer v√† ch·ªçn s·ªë chi·ªÅu l√† 17\n",
        "X_train_bow = vectorizer.fit_transform(df_x_train['message'])\n",
        "X_test_bow = vectorizer.transform(df_x_test['message'])\n",
        "\n",
        "# 8. Scale d·ªØ li·ªáu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_train_bow)\n",
        "X_test_scaled = scaler.transform(X_test_bow)\n",
        "\n",
        "# 9. Hu·∫•n luy·ªán m√¥ h√¨nh KNN v·ªõi kho·∫£ng c√°ch Cosine v√† s·ªë h√†ng x√≥m = 17\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],  # Ch·ªçn s·ªë h√†ng x√≥m = 17\n",
        "    'metric': ['cosine']  # Kho·∫£ng c√°ch cosine\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. D·ª± ƒëo√°n v√† ƒë√°nh gi√° k·∫øt qu·∫£\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaHW5W8wxaoy",
        "outputId": "40dde121-8741-4a26-defc-666bd0396a2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.8028368794326242\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.88      1566\n",
            "           1       0.74      0.38      0.50       549\n",
            "\n",
            "    accuracy                           0.80      2115\n",
            "   macro avg       0.77      0.66      0.69      2115\n",
            "weighted avg       0.79      0.80      0.78      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V·ªõi TF-IDF, ta ch·ªçn c√°c si√™u tham s·ªë k, kho·∫£ng c√°ch, s·ªë chi·ªÅu vector"
      ],
      "metadata": {
        "id": "2KWUoXGGTBvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# 2. T·∫£i d·ªØ li·ªáu t·ª´ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
        "# Ch√∫ √Ω c·∫≠p nh·∫≠t ƒë∆∞·ªùng d·∫´n ƒë√∫ng n·∫øu s·ª≠ d·ª•ng file c·ªßa b·∫°n\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 4. X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
        "# L√†m s·∫°ch d·ªØ li·ªáu\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')  # X·ª≠ l√Ω NaN\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 5. M√£ h√≥a nh√£n\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 6. H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n n√¢ng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization & remove non-alpha\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 7. So s√°nh c√°c k√≠ch th∆∞·ªõc TF-IDF kh√°c nhau v√† ch·ªçn s·ªë chi·ªÅu t·ªët nh·∫•t\n",
        "feature_sizes = [3000, 4000, 5000]  # C√°c s·ªë chi·ªÅu kh√°c nhau ƒë·ªÉ th·ª≠\n",
        "best_accuracy = 0\n",
        "best_max_feat = 0\n",
        "\n",
        "for max_feat in feature_sizes:\n",
        "    # TF-IDF vectorization v·ªõi 3-grams\n",
        "    vectorizer = TfidfVectorizer(max_features=max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "    X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "    # Scale d·ªØ li·ªáu\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    X_train_scaled = scaler.fit_transform(X_tfidf)\n",
        "    X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "    X_test_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "    # GridSearch ƒë·ªÉ t√¨m KNN t·ªët nh·∫•t\n",
        "    param_grid = {\n",
        "        'n_neighbors': [5, 9, 13, 17],\n",
        "        'metric': ['euclidean', 'cosine']\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "    # D·ª± ƒëo√°n v√† t√≠nh accuracy\n",
        "    y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(df_y_test['category'], le.inverse_transform(y_pred))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_max_feat = max_feat\n",
        "\n",
        "# Sau khi loop qua t·∫•t c·∫£ c√°c s·ªë chi·ªÅu, in ra k·∫øt qu·∫£\n",
        "print(f\"\\n==================== Best TF-IDF max_features={best_max_feat} with Accuracy={best_accuracy} ====================\")\n",
        "\n",
        "# D√πng s·ªë chi·ªÅu t·ªët nh·∫•t ƒë·ªÉ hu·∫•n luy·ªán l·∫°i m√¥ h√¨nh\n",
        "vectorizer = TfidfVectorizer(max_features=best_max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "# Scale d·ªØ li·ªáu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_tfidf)\n",
        "X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "X_test_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "# Hu·∫•n luy·ªán l·∫°i KNN v·ªõi s·ªë chi·ªÅu t·ªët nh·∫•t\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# In k·∫øt qu·∫£ cu·ªëi c√πng\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64HnqC4RVgMQ",
        "outputId": "91fb542f-8a1f-4158-f049-d4abc596f986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Best TF-IDF max_features=5000 with Accuracy=0.8959810874704491 ====================\n",
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.8959810874704491\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93      1566\n",
            "           1       0.86      0.72      0.78       549\n",
            "\n",
            "    accuracy                           0.90      2115\n",
            "   macro avg       0.88      0.84      0.86      2115\n",
            "weighted avg       0.89      0.90      0.89      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "V·ªõi Word embedding: Word2vec"
      ],
      "metadata": {
        "id": "SBziwDTjX6vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF+ SentenceStransformer+PCA"
      ],
      "metadata": {
        "id": "yNEEScX6cLFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import c√°c th∆∞ vi·ªán c·∫ßn thi·∫øt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.decomposition import PCA\n",
        "import logging\n",
        "\n",
        "# 2. T·∫Øt warning kh√¥ng c·∫ßn thi·∫øt\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"sentence_transformers.SentenceTransformer\").setLevel(logging.ERROR)\n",
        "\n",
        "# 3. T·∫£i d·ªØ li·ªáu t·ª´ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 4. ƒê·ªçc d·ªØ li·ªáu t·ª´ file CSV\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 5. X·ª≠ l√Ω d·ªØ li·ªáu ƒë·∫ßu v√†o\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 6. M√£ h√≥a nh√£n\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 7. H√†m ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n n√¢ng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 8. T·∫£i m√¥ h√¨nh embedding\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 9. Ch·ªçn gi√° tr·ªã max_features\n",
        "max_feat = 3000  # ho·∫∑c gi√° tr·ªã b·∫°n mu·ªën\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "# Sentence embedding\n",
        "X_embed = embed_model.encode(df_x_train['message'], show_progress_bar=False)\n",
        "X_embed_sparse = csr_matrix(X_embed)\n",
        "\n",
        "# Combine TF-IDF + Embedding\n",
        "X_combined = hstack([X_tfidf, X_embed_sparse])\n",
        "\n",
        "# Test data\n",
        "X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "X_test_embed = embed_model.encode(df_x_test['message'], show_progress_bar=False)\n",
        "X_test_embed_sparse = csr_matrix(X_test_embed)\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_embed_sparse])\n",
        "\n",
        "# Scale d·ªØ li·ªáu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_combined)\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "# PCA gi·∫£m chi·ªÅu xu·ªëng 200 chi·ªÅu\n",
        "pca = PCA(n_components=200)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled.toarray())\n",
        "X_test_pca = pca.transform(X_test_scaled.toarray())\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_pca, y_encoded)\n",
        "\n",
        "y_pred = grid.best_estimator_.predict(X_test_pca)\n",
        "\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNMHvSc3kktN",
        "outputId": "e26fcce3-c261-4fca-e9bb-412a0b79ad3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9361702127659575\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96      1566\n",
            "           1       0.87      0.89      0.88       549\n",
            "\n",
            "    accuracy                           0.94      2115\n",
            "   macro avg       0.92      0.92      0.92      2115\n",
            "weighted avg       0.94      0.94      0.94      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fastext Embedding"
      ],
      "metadata": {
        "id": "-9eG1T0cmJfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# T·∫£i m√¥ h√¨nh fasttext ti·∫øng Anh (wiki-news-300d-1M)\n",
        "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T42aygjym4dS",
        "outputId": "56140cc7-a3a9-4c18-f34a-6c17f8d56739"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import th∆∞ vi·ªán\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "\n",
        "# 2. T·∫Øt warning\n",
        "import logging\n",
        "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. ƒê·ªçc d·ªØ li·ªáu\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ d·ªØ li·ªáu df_x_train, df_x_test, df_y_train, df_y_test\n",
        "\n",
        "# 4. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Chuy·ªÉn t·∫•t c·∫£ th√†nh ch·ªØ th∆∞·ªùng\n",
        "    text = re.sub(r'\\d+', '', text)  # X√≥a s·ªë\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # X√≥a d·∫•u c√¢u\n",
        "    tokens = word_tokenize(text)  # T√°ch t·ª´\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization\n",
        "    return ' '.join(lemmatized)  # Tr·∫£ v·ªÅ vƒÉn b·∫£n ƒë√£ x·ª≠ l√Ω\n",
        "\n",
        "# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
        "df_x_train['message'] = df_x_train['message'].fillna('').apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].fillna('').apply(preprocess)\n",
        "\n",
        "# 5. M√£ h√≥a nh√£n\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df_y_train['category'])\n",
        "\n",
        "# 6. T·∫£i m√¥ h√¨nh FastText t·ª´ gensim\n",
        "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "# 7. Vector h√≥a vƒÉn b·∫£n v·ªõi FastText\n",
        "def get_ft_vector(text):\n",
        "    # Tokenize c√¢u\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # L·∫•y vector c·ªßa t·ª´ng t·ª´ v√† t√≠nh trung b√¨nh\n",
        "    word_vectors = []\n",
        "    for token in tokens:\n",
        "        if token in fasttext_model.key_to_index:  # Ki·ªÉm tra t·ª´ c√≥ trong m√¥ h√¨nh kh√¥ng\n",
        "            word_vectors.append(fasttext_model[token])  # L·∫•y vector c·ªßa t·ª´\n",
        "\n",
        "    # T√≠nh trung b√¨nh vector c√°c t·ª´ trong c√¢u\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)  # Trung b√¨nh c·ªßa c√°c vector t·ª´\n",
        "    else:\n",
        "        # N·∫øu kh√¥ng c√≥ t·ª´ n√†o trong m√¥ h√¨nh th√¨ tr·∫£ v·ªÅ vector 0\n",
        "        return np.zeros(fasttext_model.vector_size)\n",
        "\n",
        "# √Åp d·ª•ng cho t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
        "X_train_ft = np.array([get_ft_vector(text) for text in df_x_train['message']])\n",
        "X_test_ft = np.array([get_ft_vector(text) for text in df_x_test['message']])\n",
        "\n",
        "# 8. Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_ft)  # Chu·∫©n h√≥a cho t·∫≠p hu·∫•n luy·ªán\n",
        "X_test_scaled = scaler.transform(X_test_ft)  # Chu·∫©n h√≥a cho t·∫≠p ki·ªÉm tra\n",
        "\n",
        "# 9. Hu·∫•n luy·ªán m√¥ h√¨nh KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. D·ª± ƒëo√°n v√† ƒë√°nh gi√° k·∫øt qu·∫£\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# 11. ƒê√°nh gi√° m√¥ h√¨nh\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], le.inverse_transform(y_pred)))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJEuelKvmMCN",
        "outputId": "5826d0f9-d55f-4863-a8d0-12b08b353f29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9030732860520094\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93      1566\n",
            "           1       0.78      0.87      0.82       549\n",
            "\n",
            "    accuracy                           0.90      2115\n",
            "   macro avg       0.87      0.89      0.88      2115\n",
            "weighted avg       0.91      0.90      0.90      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN + Word2Vec"
      ],
      "metadata": {
        "id": "vNoLuqkpy1MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import th∆∞ vi·ªán\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "\n",
        "# 2. T·∫Øt warning\n",
        "import logging\n",
        "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. ƒê·ªçc d·ªØ li·ªáu\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# Gi·∫£ s·ª≠ b·∫°n ƒë√£ c√≥ d·ªØ li·ªáu df_x_train, df_x_test, df_y_train, df_y_test\n",
        "\n",
        "# 4. Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# √Åp d·ª•ng ti·ªÅn x·ª≠ l√Ω cho t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
        "df_x_train['message'] = df_x_train['message'].fillna('').apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].fillna('').apply(preprocess)\n",
        "\n",
        "# 5. M√£ h√≥a nh√£n\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df_y_train['category'])\n",
        "\n",
        "# 6. T·∫£i m√¥ h√¨nh Word2Vec t·ª´ gensim\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# 7. Vector h√≥a vƒÉn b·∫£n v·ªõi Word2Vec\n",
        "def get_w2v_vector(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    word_vectors = [word2vec_model[token] for token in tokens if token in word2vec_model.key_to_index]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "# √Åp d·ª•ng cho t·∫≠p hu·∫•n luy·ªán v√† ki·ªÉm tra\n",
        "X_train_w2v = np.array([get_w2v_vector(text) for text in df_x_train['message']])\n",
        "X_test_w2v = np.array([get_w2v_vector(text) for text in df_x_test['message']])\n",
        "\n",
        "# 8. Chu·∫©n h√≥a ƒë·∫∑c tr∆∞ng\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_w2v)\n",
        "X_test_scaled = scaler.transform(X_test_w2v)\n",
        "\n",
        "# 9. Hu·∫•n luy·ªán m√¥ h√¨nh KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. D·ª± ƒëo√°n v√† ƒë√°nh gi√° k·∫øt qu·∫£\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# 11. ƒê√°nh gi√° m√¥ h√¨nh\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], le.inverse_transform(y_pred)))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR97kHn_ym4L",
        "outputId": "f55346be-ac20-4a88-9138-70f7255a2c04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9153664302600473\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94      1566\n",
            "           1       0.80      0.89      0.85       549\n",
            "\n",
            "    accuracy                           0.92      2115\n",
            "   macro avg       0.88      0.91      0.89      2115\n",
            "weighted avg       0.92      0.92      0.92      2115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}