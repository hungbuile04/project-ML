{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ClBDMf6HQKE5",
        "outputId": "3b8a5eb5-6fe8-4eac-cab9-5817bf24b584"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gdown==4.6.0\n",
            "  Downloading gdown-4.6.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (3.18.0)\n",
            "Requirement already satisfied: requests[socks] in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (2.32.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (1.17.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.67.1)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown==4.6.0) (4.13.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown==4.6.0) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (2025.1.31)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown==4.6.0) (1.7.1)\n",
            "Downloading gdown-4.6.0-py3-none-any.whl (14 kB)\n",
            "Installing collected packages: gdown\n",
            "  Attempting uninstall: gdown\n",
            "    Found existing installation: gdown 5.2.0\n",
            "    Uninstalling gdown-5.2.0:\n",
            "      Successfully uninstalled gdown-5.2.0\n",
            "Successfully installed gdown-4.6.0\n"
          ]
        }
      ],
      "source": [
        "!pip install gdown==4.6.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Data google drive link của Hùng\n"
      ],
      "metadata": {
        "id": "KT_mjqwnQ-Mr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "!mkdir -p content/data\n",
        "# Download all files\n",
        "file_ids = [\n",
        "    '1JXdyh-UVveXhBToLJj8ZJNttiJif6zH8',  # X_train\n",
        "    '1rtyLyNDBr3sPcyElMhpDPK3qTbNZH1YT',  # X_test\n",
        "    '1IkNvEhPFfano0qSDZFWhsmpF3ttC1q0Y',  # y_train\n",
        "    '1JlMWumYQP9OIH79dVsCiZ-eavZAhgSzv'   # y_test\n",
        "]\n",
        "\n",
        "file_paths = [\n",
        "    'content/data/X_train.csv',\n",
        "    'content/data/X_test.csv',\n",
        "    'content/data/y_train.csv',\n",
        "    'content/data/y_test.csv'\n",
        "]\n",
        "for file_id, file_path in zip(file_ids, file_paths):\n",
        "    gdown.download(f'https://drive.google.com/uc?id={file_id}', file_path, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5eKWUVbRCw0",
        "outputId": "71b6bec6-437a-4837-bc07-797df2b220f4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JXdyh-UVveXhBToLJj8ZJNttiJif6zH8\n",
            "To: /content/content/data/X_train.csv\n",
            "100%|██████████| 5.03M/5.03M [00:00<00:00, 64.2MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1rtyLyNDBr3sPcyElMhpDPK3qTbNZH1YT\n",
            "To: /content/content/data/X_test.csv\n",
            "100%|██████████| 1.34M/1.34M [00:00<00:00, 20.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1IkNvEhPFfano0qSDZFWhsmpF3ttC1q0Y\n",
            "To: /content/content/data/y_train.csv\n",
            "100%|██████████| 16.9k/16.9k [00:00<00:00, 25.9MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1JlMWumYQP9OIH79dVsCiZ-eavZAhgSzv\n",
            "To: /content/content/data/y_test.csv\n",
            "100%|██████████| 4.24k/4.24k [00:00<00:00, 6.96MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kiểm tra data"
      ],
      "metadata": {
        "id": "JRLI49vgR5VO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Đọc file CSV\n",
        "import pandas as pd\n",
        "df_x_train = pd.read_csv('content/data/X_train.csv')\n",
        "df_x_test = pd.read_csv('content/data/X_test.csv')\n",
        "df_y_train = pd.read_csv('content/data/y_train.csv')\n",
        "df_y_test = pd.read_csv('content/data/y_test.csv')\n",
        "\n",
        "# Hiển thị thông tin và cột\n",
        "print(\"\\n🟦 X_train:\")\n",
        "print(df_x_train.info())\n",
        "print(\"Tên cột:\", df_x_train.columns.tolist())\n",
        "\n",
        "print(\"\\n🟨 X_test:\")\n",
        "print(df_x_test.info())\n",
        "print(\"Tên cột:\", df_x_test.columns.tolist())\n",
        "\n",
        "print(\"\\n🟩 y_train:\")\n",
        "print(df_y_train.info())\n",
        "print(\"Tên cột:\", df_y_train.columns.tolist())\n",
        "\n",
        "print(\"\\n🟥 y_test:\")\n",
        "print(df_y_test.info())\n",
        "print(\"Tên cột:\", df_y_test.columns.tolist())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOpT8T2zR7jT",
        "outputId": "69a7058d-6c7a-4d30-bff3-ca7658fb3bde"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "🟦 X_train:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8457 entries, 0 to 8456\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   message  8445 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 66.2+ KB\n",
            "None\n",
            "Tên cột: ['message']\n",
            "\n",
            "🟨 X_test:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2115 entries, 0 to 2114\n",
            "Data columns (total 1 columns):\n",
            " #   Column   Non-Null Count  Dtype \n",
            "---  ------   --------------  ----- \n",
            " 0   message  2114 non-null   object\n",
            "dtypes: object(1)\n",
            "memory usage: 16.7+ KB\n",
            "None\n",
            "Tên cột: ['message']\n",
            "\n",
            "🟩 y_train:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 8457 entries, 0 to 8456\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype\n",
            "---  ------    --------------  -----\n",
            " 0   category  8457 non-null   int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 66.2 KB\n",
            "None\n",
            "Tên cột: ['category']\n",
            "\n",
            "🟥 y_test:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 2115 entries, 0 to 2114\n",
            "Data columns (total 1 columns):\n",
            " #   Column    Non-Null Count  Dtype\n",
            "---  ------    --------------  -----\n",
            " 0   category  2115 non-null   int64\n",
            "dtypes: int64(1)\n",
            "memory usage: 16.7 KB\n",
            "None\n",
            "Tên cột: ['category']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOWs"
      ],
      "metadata": {
        "id": "j2H2NNjqxYmA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import các thư viện cần thiết\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer  # Thay đổi từ TfidfVectorizer sang CountVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# 2. Tải dữ liệu từ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. Đọc dữ liệu từ file CSV\n",
        "# Chú ý cập nhật đường dẫn đúng nếu sử dụng file của bạn\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 4. Xử lý dữ liệu đầu vào\n",
        "# Làm sạch dữ liệu\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')  # Xử lý NaN\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 5. Mã hóa nhãn\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 6. Hàm tiền xử lý văn bản nâng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization & remove non-alpha\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 7. Chuyển sang Bag of Words (BOW)\n",
        "vectorizer = CountVectorizer(max_features=17, stop_words='english')  # Sử dụng CountVectorizer và chọn số chiều là 17\n",
        "X_train_bow = vectorizer.fit_transform(df_x_train['message'])\n",
        "X_test_bow = vectorizer.transform(df_x_test['message'])\n",
        "\n",
        "# 8. Scale dữ liệu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_train_bow)\n",
        "X_test_scaled = scaler.transform(X_test_bow)\n",
        "\n",
        "# 9. Huấn luyện mô hình KNN với khoảng cách Cosine và số hàng xóm = 17\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],  # Chọn số hàng xóm = 17\n",
        "    'metric': ['cosine']  # Khoảng cách cosine\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. Dự đoán và đánh giá kết quả\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XaHW5W8wxaoy",
        "outputId": "40dde121-8741-4a26-defc-666bd0396a2f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.8028368794326242\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.81      0.95      0.88      1566\n",
            "           1       0.74      0.38      0.50       549\n",
            "\n",
            "    accuracy                           0.80      2115\n",
            "   macro avg       0.77      0.66      0.69      2115\n",
            "weighted avg       0.79      0.80      0.78      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với TF-IDF, ta chọn các siêu tham số k, khoảng cách, số chiều vector"
      ],
      "metadata": {
        "id": "2KWUoXGGTBvF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import các thư viện cần thiết\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "# 2. Tải dữ liệu từ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. Đọc dữ liệu từ file CSV\n",
        "# Chú ý cập nhật đường dẫn đúng nếu sử dụng file của bạn\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 4. Xử lý dữ liệu đầu vào\n",
        "# Làm sạch dữ liệu\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')  # Xử lý NaN\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 5. Mã hóa nhãn\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 6. Hàm tiền xử lý văn bản nâng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Lowercasing\n",
        "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Remove punctuation\n",
        "    tokens = word_tokenize(text)  # Tokenize\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization & remove non-alpha\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 7. So sánh các kích thước TF-IDF khác nhau và chọn số chiều tốt nhất\n",
        "feature_sizes = [3000, 4000, 5000]  # Các số chiều khác nhau để thử\n",
        "best_accuracy = 0\n",
        "best_max_feat = 0\n",
        "\n",
        "for max_feat in feature_sizes:\n",
        "    # TF-IDF vectorization với 3-grams\n",
        "    vectorizer = TfidfVectorizer(max_features=max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "    X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "    # Scale dữ liệu\n",
        "    scaler = StandardScaler(with_mean=False)\n",
        "    X_train_scaled = scaler.fit_transform(X_tfidf)\n",
        "    X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "    X_test_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "    # GridSearch để tìm KNN tốt nhất\n",
        "    param_grid = {\n",
        "        'n_neighbors': [5, 9, 13, 17],\n",
        "        'metric': ['euclidean', 'cosine']\n",
        "    }\n",
        "\n",
        "    grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "    grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "    # Dự đoán và tính accuracy\n",
        "    y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "    accuracy = accuracy_score(df_y_test['category'], le.inverse_transform(y_pred))\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_max_feat = max_feat\n",
        "\n",
        "# Sau khi loop qua tất cả các số chiều, in ra kết quả\n",
        "print(f\"\\n==================== Best TF-IDF max_features={best_max_feat} with Accuracy={best_accuracy} ====================\")\n",
        "\n",
        "# Dùng số chiều tốt nhất để huấn luyện lại mô hình\n",
        "vectorizer = TfidfVectorizer(max_features=best_max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "# Scale dữ liệu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_tfidf)\n",
        "X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "X_test_scaled = scaler.transform(X_test_tfidf)\n",
        "\n",
        "# Huấn luyện lại KNN với số chiều tốt nhất\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# In kết quả cuối cùng\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64HnqC4RVgMQ",
        "outputId": "91fb542f-8a1f-4158-f049-d4abc596f986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================== Best TF-IDF max_features=5000 with Accuracy=0.8959810874704491 ====================\n",
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.8959810874704491\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.91      0.96      0.93      1566\n",
            "           1       0.86      0.72      0.78       549\n",
            "\n",
            "    accuracy                           0.90      2115\n",
            "   macro avg       0.88      0.84      0.86      2115\n",
            "weighted avg       0.89      0.90      0.89      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Với Word embedding: Word2vec"
      ],
      "metadata": {
        "id": "SBziwDTjX6vh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TF-IDF+ SentenceStransformer+PCA"
      ],
      "metadata": {
        "id": "yNEEScX6cLFM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import các thư viện cần thiết\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from scipy.sparse import csr_matrix, hstack\n",
        "from sklearn.decomposition import PCA\n",
        "import logging\n",
        "\n",
        "# 2. Tắt warning không cần thiết\n",
        "logging.getLogger(\"huggingface_hub\").setLevel(logging.ERROR)\n",
        "logging.getLogger(\"sentence_transformers.SentenceTransformer\").setLevel(logging.ERROR)\n",
        "\n",
        "# 3. Tải dữ liệu từ nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 4. Đọc dữ liệu từ file CSV\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# 5. Xử lý dữ liệu đầu vào\n",
        "df_x_train['message'] = df_x_train['message'].fillna('')\n",
        "df_x_test['message'] = df_x_test['message'].fillna('')\n",
        "X_raw = df_x_train['message']\n",
        "y = df_y_train['category']\n",
        "\n",
        "# 6. Mã hóa nhãn\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(y)\n",
        "\n",
        "# 7. Hàm tiền xử lý văn bản nâng cao\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "df_x_train['message'] = df_x_train['message'].apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].apply(preprocess)\n",
        "\n",
        "# 8. Tải mô hình embedding\n",
        "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# 9. Chọn giá trị max_features\n",
        "max_feat = 3000  # hoặc giá trị bạn muốn\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(max_features=max_feat, stop_words='english', ngram_range=(1, 3))\n",
        "X_tfidf = vectorizer.fit_transform(df_x_train['message'])\n",
        "\n",
        "# Sentence embedding\n",
        "X_embed = embed_model.encode(df_x_train['message'], show_progress_bar=False)\n",
        "X_embed_sparse = csr_matrix(X_embed)\n",
        "\n",
        "# Combine TF-IDF + Embedding\n",
        "X_combined = hstack([X_tfidf, X_embed_sparse])\n",
        "\n",
        "# Test data\n",
        "X_test_tfidf = vectorizer.transform(df_x_test['message'])\n",
        "X_test_embed = embed_model.encode(df_x_test['message'], show_progress_bar=False)\n",
        "X_test_embed_sparse = csr_matrix(X_test_embed)\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_embed_sparse])\n",
        "\n",
        "# Scale dữ liệu\n",
        "scaler = StandardScaler(with_mean=False)\n",
        "X_train_scaled = scaler.fit_transform(X_combined)\n",
        "X_test_scaled = scaler.transform(X_test_combined)\n",
        "\n",
        "# PCA giảm chiều xuống 200 chiều\n",
        "pca = PCA(n_components=200)\n",
        "X_train_pca = pca.fit_transform(X_train_scaled.toarray())\n",
        "X_test_pca = pca.transform(X_test_scaled.toarray())\n",
        "\n",
        "# Grid search\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_pca, y_encoded)\n",
        "\n",
        "y_pred = grid.best_estimator_.predict(X_test_pca)\n",
        "\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNMHvSc3kktN",
        "outputId": "e26fcce3-c261-4fca-e9bb-412a0b79ad3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9361702127659575\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.95      0.96      1566\n",
            "           1       0.87      0.89      0.88       549\n",
            "\n",
            "    accuracy                           0.94      2115\n",
            "   macro avg       0.92      0.92      0.92      2115\n",
            "weighted avg       0.94      0.94      0.94      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fastext Embedding"
      ],
      "metadata": {
        "id": "-9eG1T0cmJfj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim.downloader as api\n",
        "\n",
        "# Tải mô hình fasttext tiếng Anh (wiki-news-300d-1M)\n",
        "fasttext_model = api.load('fasttext-wiki-news-subwords-300')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T42aygjym4dS",
        "outputId": "56140cc7-a3a9-4c18-f34a-6c17f8d56739"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import thư viện\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "\n",
        "# 2. Tắt warning\n",
        "import logging\n",
        "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. Đọc dữ liệu\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# Giả sử bạn đã có dữ liệu df_x_train, df_x_test, df_y_train, df_y_test\n",
        "\n",
        "# 4. Tiền xử lý văn bản\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()  # Chuyển tất cả thành chữ thường\n",
        "    text = re.sub(r'\\d+', '', text)  # Xóa số\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))  # Xóa dấu câu\n",
        "    tokens = word_tokenize(text)  # Tách từ\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]  # Lemmatization\n",
        "    return ' '.join(lemmatized)  # Trả về văn bản đã xử lý\n",
        "\n",
        "# Áp dụng tiền xử lý cho tập huấn luyện và kiểm tra\n",
        "df_x_train['message'] = df_x_train['message'].fillna('').apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].fillna('').apply(preprocess)\n",
        "\n",
        "# 5. Mã hóa nhãn\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df_y_train['category'])\n",
        "\n",
        "# 6. Tải mô hình FastText từ gensim\n",
        "fasttext_model = api.load('fasttext-wiki-news-subwords-300')\n",
        "\n",
        "# 7. Vector hóa văn bản với FastText\n",
        "def get_ft_vector(text):\n",
        "    # Tokenize câu\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Lấy vector của từng từ và tính trung bình\n",
        "    word_vectors = []\n",
        "    for token in tokens:\n",
        "        if token in fasttext_model.key_to_index:  # Kiểm tra từ có trong mô hình không\n",
        "            word_vectors.append(fasttext_model[token])  # Lấy vector của từ\n",
        "\n",
        "    # Tính trung bình vector các từ trong câu\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)  # Trung bình của các vector từ\n",
        "    else:\n",
        "        # Nếu không có từ nào trong mô hình thì trả về vector 0\n",
        "        return np.zeros(fasttext_model.vector_size)\n",
        "\n",
        "# Áp dụng cho tập huấn luyện và kiểm tra\n",
        "X_train_ft = np.array([get_ft_vector(text) for text in df_x_train['message']])\n",
        "X_test_ft = np.array([get_ft_vector(text) for text in df_x_test['message']])\n",
        "\n",
        "# 8. Chuẩn hóa đặc trưng\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_ft)  # Chuẩn hóa cho tập huấn luyện\n",
        "X_test_scaled = scaler.transform(X_test_ft)  # Chuẩn hóa cho tập kiểm tra\n",
        "\n",
        "# 9. Huấn luyện mô hình KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. Dự đoán và đánh giá kết quả\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# 11. Đánh giá mô hình\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], le.inverse_transform(y_pred)))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJEuelKvmMCN",
        "outputId": "5826d0f9-d55f-4863-a8d0-12b08b353f29"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9030732860520094\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.95      0.91      0.93      1566\n",
            "           1       0.78      0.87      0.82       549\n",
            "\n",
            "    accuracy                           0.90      2115\n",
            "   macro avg       0.87      0.89      0.88      2115\n",
            "weighted avg       0.91      0.90      0.90      2115\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN + Word2Vec"
      ],
      "metadata": {
        "id": "vNoLuqkpy1MD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Import thư viện\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "import re\n",
        "import gensim.downloader as api\n",
        "\n",
        "# 2. Tắt warning\n",
        "import logging\n",
        "logging.getLogger(\"gensim\").setLevel(logging.ERROR)\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "# 3. Đọc dữ liệu\n",
        "# df_x_train = pd.read_csv('path_to_X_train.csv')\n",
        "# df_x_test = pd.read_csv('path_to_X_test.csv')\n",
        "# df_y_train = pd.read_csv('path_to_y_train.csv')\n",
        "# df_y_test = pd.read_csv('path_to_y_test.csv')\n",
        "\n",
        "# Giả sử bạn đã có dữ liệu df_x_train, df_x_test, df_y_train, df_y_test\n",
        "\n",
        "# 4. Tiền xử lý văn bản\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def preprocess(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "    tokens = word_tokenize(text)\n",
        "    lemmatized = [lemmatizer.lemmatize(token) for token in tokens if token.isalpha()]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "# Áp dụng tiền xử lý cho tập huấn luyện và kiểm tra\n",
        "df_x_train['message'] = df_x_train['message'].fillna('').apply(preprocess)\n",
        "df_x_test['message'] = df_x_test['message'].fillna('').apply(preprocess)\n",
        "\n",
        "# 5. Mã hóa nhãn\n",
        "le = LabelEncoder()\n",
        "y_encoded = le.fit_transform(df_y_train['category'])\n",
        "\n",
        "# 6. Tải mô hình Word2Vec từ gensim\n",
        "word2vec_model = api.load('word2vec-google-news-300')\n",
        "\n",
        "# 7. Vector hóa văn bản với Word2Vec\n",
        "def get_w2v_vector(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    word_vectors = [word2vec_model[token] for token in tokens if token in word2vec_model.key_to_index]\n",
        "    if word_vectors:\n",
        "        return np.mean(word_vectors, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "# Áp dụng cho tập huấn luyện và kiểm tra\n",
        "X_train_w2v = np.array([get_w2v_vector(text) for text in df_x_train['message']])\n",
        "X_test_w2v = np.array([get_w2v_vector(text) for text in df_x_test['message']])\n",
        "\n",
        "# 8. Chuẩn hóa đặc trưng\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_w2v)\n",
        "X_test_scaled = scaler.transform(X_test_w2v)\n",
        "\n",
        "# 9. Huấn luyện mô hình KNN\n",
        "param_grid = {\n",
        "    'n_neighbors': [17],\n",
        "    'metric': ['cosine']\n",
        "}\n",
        "grid = GridSearchCV(KNeighborsClassifier(), param_grid, cv=3, scoring='accuracy', n_jobs=-1)\n",
        "grid.fit(X_train_scaled, y_encoded)\n",
        "\n",
        "# 10. Dự đoán và đánh giá kết quả\n",
        "y_pred = grid.best_estimator_.predict(X_test_scaled)\n",
        "\n",
        "# 11. Đánh giá mô hình\n",
        "print(\"Best KNN Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy:\", accuracy_score(df_y_test['category'], le.inverse_transform(y_pred)))\n",
        "print(\"Classification Report:\\n\", classification_report(le.inverse_transform(df_y_test['category']), le.inverse_transform(y_pred)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JR97kHn_ym4L",
        "outputId": "f55346be-ac20-4a88-9138-70f7255a2c04"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best KNN Parameters: {'metric': 'cosine', 'n_neighbors': 17}\n",
            "Accuracy: 0.9153664302600473\n",
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.92      0.94      1566\n",
            "           1       0.80      0.89      0.85       549\n",
            "\n",
            "    accuracy                           0.92      2115\n",
            "   macro avg       0.88      0.91      0.89      2115\n",
            "weighted avg       0.92      0.92      0.92      2115\n",
            "\n"
          ]
        }
      ]
    }
  ]
}